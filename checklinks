#!/bin/bash
set -eu


# Check for dead links
#
# Fail when website links to any broken urls.
# Gather all internal pages first,
# then check their linked ressources.
# Failed links need to be checked again with a GET request,
# since some of them just don't support HEAD.


domain="jorin.me"
common="
  --no-verbose
  --no-directories
  --wait 0.1
  --timeout 30
  --tries 3
  --user-agent linkchecker
  --execute robots=off
"
spider="
  --spider
  --recursive
"
extern="
  --level 1
  --span-hosts
  --exclude-domains $domain
"
nohead="
  --output-document /dev/null
"

printf "Finding all internal pages\n"
pages="$(mktemp)"
wget $common $spider $domain 2>&1 | tee >(grep -o "https:\/\/$domain[^ ]*" > "$pages")

printf "\n\nChecking all external links\n"
brokenlinks="$(mktemp)"
wget $common $spider $extern -i "$pages" 2>&1 | tee >(grep "^http" | cut -d: -f1-2 > "$brokenlinks") || echo
wget $common $nohead -i "$brokenlinks" 2>&1

rm "$brokenlinks" "$pages"
